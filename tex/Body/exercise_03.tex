Implement the sampler and show its diagnostics.

The full MCMC sampler described in Problem 2 was implemented in R. The code is publicly available at:

\begin{quote}
\url{https://github.com/nicolasdemoura/Bayesian-Econometrics---Problem-Set-2}
\end{quote}

The sampler was run for 10,000 iterations with a burn-in of 500. Diagnostics were computed for representative parameters: the decay parameter $\lambda$, the autoregressive coefficient $A_{11}$, selected elements of the variance matrices $\mathbf{H}$ and $\mathbf{Q}$ and the mean vector $\boldsymbol{\mu}$.

\paragraph{Trace plots}
The trace plots below show good convergence behavior and mixing across iterations.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../figures/trace_lambda.png}
    \includegraphics[width=0.45\textwidth]{../figures/trace_A11.png}
    \includegraphics[width=0.45\textwidth]{../figures/trace_H11.png}
    \includegraphics[width=0.45\textwidth]{../figures/trace_Q11.png}
    \includegraphics[width=0.45\textwidth]{../figures/trace_mu1.png}
    \caption{Trace plots for $\lambda$, $A_{11}$, $H_{11}$, $Q_{11}$ and $\mu_1$.}
\end{figure}

\paragraph{Autocorrelation plots} Autocorrelation plots confirm that dependence across samples is moderate, and that the sampler is producing informative posterior draws. The two variables with the highest autocorrelation are $\lambda$ and $A_{11}$, which is expected given their strong persistence of the underlying MCMC process. The autocorrelation for the other parameters is lower, indicating that they are less persistent.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{../figures/acf_lambda.png}
    \includegraphics[width=0.45\textwidth]{../figures/acf_A11.png}
    \includegraphics[width=0.45\textwidth]{../figures/acf_H11.png}
    \includegraphics[width=0.45\textwidth]{../figures/acf_Q11.png}
    
    \caption{Autocorrelation plots for $\lambda$, $A_{11}$, $H_{11}$, $Q_{11}$ and $\mu_1$.}
\end{figure}

\paragraph{Acceptance rates and effective sample size} The table below summarizes acceptance rates for the Metropolis-Hastings steps (for $\lambda$ and $\mathbf{A}$) and the effective sample size (ESS) for selected components. Values are consistent with good sampler performance, with acceptance rates around 20\% for the Metropolis-Hastings steps and ESS values indicating that the sampler is producing independent samples. The effective sample size is computed as $ESS = \frac{N}{1 + 2\sum_{k=1}^{\infty} \rho(k)}$, where $\rho(k)$ is the autocorrelation at lag $k$ and $N$ is the total number of samples.

\input{../tables/acceptance_rates.tex}